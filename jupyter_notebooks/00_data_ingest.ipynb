{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Data Ingestion Pipeline\n",
    "## inDrive Hackathon 2024 - Case 2: Geotracks Analysis\n",
    "\n",
    "This notebook handles the ingestion and initial validation of anonymized trip geotrack data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Validation libraries\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Data Ingestion Pipeline - Ready!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected data schema\n",
    "EXPECTED_COLUMNS = {\n",
    "    'trip_id': 'object',\n",
    "    'timestamp': 'object',  # Will be converted to datetime\n",
    "    'lat': 'float64',\n",
    "    'lon': 'float64'\n",
    "}\n",
    "\n",
    "# Data quality constraints\n",
    "DATA_CONSTRAINTS = {\n",
    "    'lat_range': (-90.0, 90.0),\n",
    "    'lon_range': (-180.0, 180.0),\n",
    "    'min_trip_duration': 60,  # seconds\n",
    "    'max_trip_duration': 7200,  # 2 hours\n",
    "    'min_speed': 0.5,  # km/h\n",
    "    'max_speed': 150.0  # km/h\n",
    "}\n",
    "\n",
    "def validate_schema(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"Validate that DataFrame matches expected schema\"\"\"\n",
    "    \n",
    "    # Check required columns\n",
    "    missing_cols = set(EXPECTED_COLUMNS.keys()) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing required columns: {missing_cols}\")\n",
    "        return False\n",
    "    \n",
    "    # Check data types (after conversion)\n",
    "    for col, expected_type in EXPECTED_COLUMNS.items():\n",
    "        if col == 'timestamp':\n",
    "            continue  # Will be converted separately\n",
    "        \n",
    "        if not pd.api.types.is_dtype_equal(df[col].dtype, expected_type):\n",
    "            logger.warning(f\"Column {col} has type {df[col].dtype}, expected {expected_type}\")\n",
    "    \n",
    "    logger.info(\"Schema validation passed\")\n",
    "    return True\n",
    "\n",
    "print(\"Schema validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trip_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load trip data from CSV file with validation\"\"\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Loading data from: {file_path}\")\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        logger.info(f\"Loaded {len(df)} records\")\n",
    "        \n",
    "        # Validate schema\n",
    "        if not validate_schema(df):\n",
    "            raise ValueError(\"Schema validation failed\")\n",
    "        \n",
    "        # Convert timestamp\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        logger.info(\"Timestamp conversion successful\")\n",
    "        \n",
    "        # Basic data quality checks\n",
    "        initial_count = len(df)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['trip_id', 'timestamp'])\n",
    "        if len(df) < initial_count:\n",
    "            logger.warning(f\"Removed {initial_count - len(df)} duplicate records\")\n",
    "        \n",
    "        # Remove invalid coordinates\n",
    "        lat_min, lat_max = DATA_CONSTRAINTS['lat_range']\n",
    "        lon_min, lon_max = DATA_CONSTRAINTS['lon_range']\n",
    "        \n",
    "        valid_coords = (\n",
    "            (df['lat'] >= lat_min) & (df['lat'] <= lat_max) &\n",
    "            (df['lon'] >= lon_min) & (df['lon'] <= lon_max)\n",
    "        )\n",
    "        \n",
    "        invalid_count = (~valid_coords).sum()\n",
    "        if invalid_count > 0:\n",
    "            logger.warning(f\"Removed {invalid_count} records with invalid coordinates\")\n",
    "            df = df[valid_coords]\n",
    "        \n",
    "        logger.info(f\"Data loading complete. Final count: {len(df)} records\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate_sample_data(n_trips: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"Generate sample trip data for demonstration purposes\"\"\"\n",
    "    \n",
    "    logger.info(f\"Generating {n_trips} sample trips\")\n",
    "    \n",
    "    # Moscow coordinates as base\n",
    "    center_lat, center_lon = 55.7558, 37.6176\n",
    "    \n",
    "    # Generate trips\n",
    "    trips = []\n",
    "    \n",
    "    for i in range(n_trips):\n",
    "        trip_id = f\"T-2024-{i:06d}\"\n",
    "        \n",
    "        # Random timestamp in the last 7 days\n",
    "        base_time = datetime.now() - timedelta(days=7)\n",
    "        random_seconds = np.random.randint(0, 7*24*3600)\n",
    "        timestamp = base_time + timedelta(seconds=random_seconds)\n",
    "        \n",
    "        # Generate realistic GPS coordinates\n",
    "        # Create clusters around different city areas\n",
    "        cluster_type = i % 5\n",
    "        \n",
    "        if cluster_type == 0:  # Airport area\n",
    "            lat_offset = np.random.normal(0.15, 0.02)\n",
    "            lon_offset = np.random.normal(0.20, 0.02)\n",
    "        elif cluster_type == 1:  # City center\n",
    "            lat_offset = np.random.normal(0, 0.01)\n",
    "            lon_offset = np.random.normal(0, 0.01)\n",
    "        elif cluster_type == 2:  # Business district\n",
    "            lat_offset = np.random.normal(-0.05, 0.02)\n",
    "            lon_offset = np.random.normal(-0.10, 0.02)\n",
    "        elif cluster_type == 3:  # Residential north\n",
    "            lat_offset = np.random.normal(0.08, 0.03)\n",
    "            lon_offset = np.random.normal(-0.05, 0.03)\n",
    "        else:  # Shopping/entertainment\n",
    "            lat_offset = np.random.normal(-0.02, 0.015)\n",
    "            lon_offset = np.random.normal(0.08, 0.02)\n",
    "        \n",
    "        lat = center_lat + lat_offset\n",
    "        lon = center_lon + lon_offset\n",
    "        \n",
    "        trips.append({\n",
    "            'trip_id': trip_id,\n",
    "            'timestamp': timestamp,\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(trips)\n",
    "    logger.info(f\"Generated {len(df)} sample trips\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load real data, fall back to sample data\n",
    "data_file_path = \"../data/trip_data.csv\"  # Adjust path as needed\n",
    "\n",
    "try:\n",
    "    # Attempt to load real data\n",
    "    df_trips = load_trip_data(data_file_path)\n",
    "    data_source = \"real\"\n",
    "except (FileNotFoundError, Exception) as e:\n",
    "    logger.info(f\"Real data not available ({e}), generating sample data\")\n",
    "    df_trips = generate_sample_data(n_trips=2000)\n",
    "    data_source = \"sample\"\n",
    "\n",
    "print(f\"\\n=== DATA SUMMARY ====\")\n",
    "print(f\"Data source: {data_source}\")\n",
    "print(f\"Total records: {len(df_trips):,}\")\n",
    "print(f\"Date range: {df_trips['timestamp'].min()} to {df_trips['timestamp'].max()}\")\n",
    "print(f\"Unique trips: {df_trips['trip_id'].nunique():,}\")\n",
    "print(f\"Columns: {list(df_trips.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    assessment = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    assessment['total_records'] = len(df)\n",
    "    assessment['unique_trips'] = df['trip_id'].nunique()\n",
    "    assessment['date_range'] = (df['timestamp'].min(), df['timestamp'].max())\n",
    "    \n",
    "    # Missing values\n",
    "    assessment['missing_values'] = df.isnull().sum().to_dict()\n",
    "    \n",
    "    # Coordinate ranges\n",
    "    assessment['lat_range'] = (df['lat'].min(), df['lat'].max())\n",
    "    assessment['lon_range'] = (df['lon'].min(), df['lon'].max())\n",
    "    \n",
    "    # Temporal distribution\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    assessment['hourly_distribution'] = df['hour'].value_counts().sort_index().to_dict()\n",
    "    assessment['daily_distribution'] = df['day_of_week'].value_counts().sort_index().to_dict()\n",
    "    \n",
    "    # Geographic distribution\n",
    "    assessment['coordinate_std'] = {\n",
    "        'lat_std': df['lat'].std(),\n",
    "        'lon_std': df['lon'].std()\n",
    "    }\n",
    "    \n",
    "    return assessment\n",
    "\n",
    "# Perform quality assessment\n",
    "quality_report = assess_data_quality(df_trips)\n",
    "\n",
    "print(\"\\n=== DATA QUALITY REPORT ====\")\n",
    "print(f\"Total records: {quality_report['total_records']:,}\")\n",
    "print(f\"Unique trips: {quality_report['unique_trips']:,}\")\n",
    "print(f\"Missing values: {quality_report['missing_values']}\")\n",
    "print(f\"Latitude range: {quality_report['lat_range'][0]:.4f} to {quality_report['lat_range'][1]:.4f}\")\n",
    "print(f\"Longitude range: {quality_report['lon_range'][0]:.4f} to {quality_report['lon_range'][1]:.4f}\")\n",
    "print(f\"Geographic spread (std): lat={quality_report['coordinate_std']['lat_std']:.4f}, lon={quality_report['coordinate_std']['lon_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Geographic distribution\n",
    "axes[0, 0].scatter(df_trips['lon'], df_trips['lat'], alpha=0.5, s=1)\n",
    "axes[0, 0].set_xlabel('Longitude')\n",
    "axes[0, 0].set_ylabel('Latitude')\n",
    "axes[0, 0].set_title('Geographic Distribution of Trips')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Temporal distribution by hour\n",
    "hourly_counts = df_trips['timestamp'].dt.hour.value_counts().sort_index()\n",
    "axes[0, 1].bar(hourly_counts.index, hourly_counts.values)\n",
    "axes[0, 1].set_xlabel('Hour of Day')\n",
    "axes[0, 1].set_ylabel('Number of Trips')\n",
    "axes[0, 1].set_title('Trip Distribution by Hour')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Daily distribution\n",
    "daily_counts = df_trips['timestamp'].dt.dayofweek.value_counts().sort_index()\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[1, 0].bar(range(len(daily_counts)), daily_counts.values)\n",
    "axes[1, 0].set_xticks(range(len(day_names)))\n",
    "axes[1, 0].set_xticklabels(day_names)\n",
    "axes[1, 0].set_xlabel('Day of Week')\n",
    "axes[1, 0].set_ylabel('Number of Trips')\n",
    "axes[1, 0].set_title('Trip Distribution by Day of Week')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Coordinate density heatmap\n",
    "try:\n",
    "    # Sample data for heatmap if too many points\n",
    "    sample_df = df_trips.sample(min(1000, len(df_trips)))\n",
    "    \n",
    "    # Create 2D histogram\n",
    "    h = axes[1, 1].hist2d(sample_df['lon'], sample_df['lat'], bins=50, cmap='YlOrRd')\n",
    "    axes[1, 1].set_xlabel('Longitude')\n",
    "    axes[1, 1].set_ylabel('Latitude')\n",
    "    axes[1, 1].set_title('Trip Density Heatmap')\n",
    "    plt.colorbar(h[3], ax=axes[1, 1])\n",
    "except Exception as e:\n",
    "    axes[1, 1].text(0.5, 0.5, f'Heatmap error: {str(e)}', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nData visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data for next steps\n",
    "output_path = \"../data/processed/01_ingested_trips.csv\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Save processed data\n",
    "df_trips.to_csv(output_path, index=False)\n",
    "logger.info(f\"Processed data saved to: {output_path}\")\n",
    "\n",
    "# Save quality report\n",
    "import json\n",
    "quality_report_path = \"../data/processed/01_quality_report.json\"\n",
    "\n",
    "# Convert datetime objects for JSON serialization\n",
    "quality_report_json = quality_report.copy()\n",
    "quality_report_json['date_range'] = [\n",
    "    quality_report['date_range'][0].isoformat(),\n",
    "    quality_report['date_range'][1].isoformat()\n",
    "]\n",
    "\n",
    "with open(quality_report_path, 'w') as f:\n",
    "    json.dump(quality_report_json, f, indent=2)\n",
    "    \n",
    "logger.info(f\"Quality report saved to: {quality_report_path}\")\n",
    "\n",
    "print(\"\\n=== INGESTION COMPLETE ====\")\n",
    "print(f\"✅ {len(df_trips):,} trips processed successfully\")\n",
    "print(f\"✅ Data exported to: {output_path}\")\n",
    "print(f\"✅ Quality report saved to: {quality_report_path}\")\n",
    "print(\"\\nReady for next step: 01_preprocessing.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}